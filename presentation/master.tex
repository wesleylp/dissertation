%******************************************************
% Header
%******************************************************

%Document type
\documentclass{beamer}

%Theme
%\usetheme{Madrid}

%Packages
\usepackage{etex}
\usepackage{mypresentation}							%SMT presentation style
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
% \usepackage[utf8]{inputenc}							%Language/input coding
% \usepackage[brazil, english]{babel}					%Brazilian Portuguese Language Package
% \usepackage[T1]{fontenc}							%Hyphenation
% \usepackage{graphicx}								%Graphic package
% %\usepackage{subfig}									%Subfloats
% \usepackage{color}									%Color package
% \usepackage{lmodern}								%Solve font problems
\usepackage{amssymb}								%Math symbols
\usepackage{amsthm}									%Theorem
% \usepackage{multimedia}								%Multimedia package
% \usepackage{array}									%For tabular
%\usepackage{pgfplotstable}								%For extracting tables from csv
\usepackage{tikz}										%Tikz graphic library
\usetikzlibrary{patterns, calc}							%Tikz patterns and computations

\usepackage{multirow}
\usepackage{multicol}
\usepackage{booktabs}
\usepackage{subfig}
\usepackage{myMacros}
\usepackage{hyperref}

\graphicspath{{/home/wesley/Dropbox/Mestrado/dissertation/text/v0/images/}}

%******************************************************
% Math operators
%******************************************************
% \DeclareMathOperator{\tr}{tr}								%Trace
% \DeclareMathOperator{\diag}{diag}							%Diag
% \DeclareMathOperator*{\argmax}{arg\,max}					%Argmax
% \DeclareMathOperator*{\argmin}{arg\,min}					%Argmin
% \DeclareMathOperator{\sgn}{sgn}								%Signal function
% \newcommand{\Trm}{\mathrm{T}}								%Transpose
% \newcommand{\drm}{\mathrm{d}}								%Differential
% \newcommand{\erm}{\mathrm{e}}								%Exponential
% \newcommand{\jrm}{\mathrm{j}}								%Complex number

\newcommand\blfootnote[1]{%
	\begingroup
	\renewcommand\thefootnote{}\footnote{#1}%
	\addtocounter{footnote}{-1}%
	\endgroup
}


%******************************************************
% Title page
%******************************************************

%Title and subtitle
\title[Wesley L. Passos]{{\huge {Automatic {\it Aedes aegypti} Breeding Grounds Detection Using Computer Vision Techniques} }
	}

%Author
\author[]{
	Wesley Lobato Passos\\
	\vspace{5mm}
	\footnotesize{Eduardo A. B. da Silva \&
	Gabriel M. Araujo}
}



%Institute
\institute[SMT/COPPE/UFRJ]
{
	Signals, Multimedia, and Telecommunications Laboratory \newline
	COPPE/UFRJ
}

\date{February 27, 2019}
%******************************************************
% Section open
%******************************************************

\AtBeginSection[]
{
	\begin{frame}
		\frametitle{Sumário}
		%\tableofcontents[currentsection, hideallsubsections]
		\tableofcontents[currentsection]
	\end{frame}
}

%******************************************************
% Main Body
%******************************************************

%Document Beginning
\begin{document}

  %% Front page logos.
  \MyLogos{0.60cm}

  %Title frame
  \begin{frame}
    \titlepage
  \end{frame}

  %% Smaller logos for the other slides.
  \MyLogos{0.30cm}

  %Table of Contents
  \begin{frame}
    \frametitle{Outline}
    \tableofcontents[hideallsubsections]
  \end{frame}



	%%============================================================================
	%% SECTION - Introduction
	%%============================================================================
	\section{Introduction}

		\begin{frame}\frametitle{Motivation}

			\begin{itemize}
				\item The arboviruses ({\bf dengue}, {\bf chikungunya}, {\bf zika}, and {\bf yellow fever})
				are one of the leading global health problem;
				\item the {\it Aedes aegypti} is the main {\bf vector} of such diseases;
				\item the best form of combat is {\bf eliminating} possible mosquito {\bf foci} proliferation;
				\item it can be {\bf expensive} and {\bf time-consuming} (therefore, {\bf inefficient});
				\item to use {\bf aerial images} and {\bf videos} captured by an {\bf UAV};
				\item {\bf automatic detection} of regions and objects with {potential mosquito breeding sites} in order to assist health agents;
				\item to apply {\bf computer vision} and {\bf machine learning} techniques.
		\end{itemize}

	\end{frame}


	%%==============================================================================
	%% SECTION - The Aedes aegypti
	%%==============================================================================
	\section{The Aedes aegypti}

	\begin{frame}
		\frametitle{Action of the Health's Department}

		The Health's Department follows an \textbf{internacional protocol} to combat arboviruses, in which they have routine actions, such as daily visits and strategic points, and emergency actions, when there is a case report.
		\newline

		The control methods are divided into
		\begin{itemize}
			\item Mechanical: displacement of object, sealing of water tank, etc
			\item Chemical: larvicide and insecticide/smoke repellency
			\item Biological: larvae and wolbachia
			\item Legal: with the support of justice
			\item Integrated: involving other secretariats
		\end{itemize}

		\vspace{3.5mm}
		The goal is to keep vector control below 1\%.
	\end{frame}

	\begin{frame}
		\frametitle{Final Product for the Government}

		We expect to deliver a \textbf{decision support system} that performs a scan in a predefined area and can identify and classify possible breeding sites according to the codes defined in the control model used by SVS.
		\newline

		In this way, indicate your \textbf{geographic positions} so that the expert evaluates and defines the actions to be taken in the property he identified.
		\newline

		The \textbf{main advantage} for the secretariat is in the legal control, when there is case notification and a property near the locality is abandoned or blocked.
	\end{frame}

	\begin{frame}
		\frametitle{Objects of Interest}


		\begin{table}[]
			\resizebox{\textwidth}{!}{%
			\begin{tabular}{@{}cc@{}}
				\toprule
				\textbf{Code} & \textbf{Description}                                                       \\ \midrule
				A1            & Water tank connected to the grid (high tanks)                              \\
				A2            & Deposits at ground level (barrel, tub, drum, tank, well)                   \\
				B             & Mobile containers (vases/jars, plates, drippings, drinking fountains, etc) \\
				C             & Fixed deposits (tanks, gutters, slabs, etc.)                               \\
				D1            & Tires and other rolling materials                                          \\
				D2            & Garbage (plastic containers, bottles, cans, scraps)                        \\
				E             & Natural deposits (bromeliads, bark, tree holes)                            \\ \bottomrule
				\end{tabular}
				}
		\end{table}
	\end{frame}




%%==============================================================================
%% SECTION - Vision Meets Unmanned Vehicles
%%==============================================================================
\section{Vision Meets Unmanned Vehicles}

	\subsection{Related works}

	\subsection{The CEFET dataset}

	\subsection{The MBG dataset}

		\begin{frame}\frametitle{}
			Due to the nature of our problem, we decided to use
			\begin{itemize}
				\item the downward vision system
				\item constant drone speed and height
				\item 4K (3.840 x 2.160) resolution and H.265 (maximum $30fps$)
			\end{itemize}

			\bigskip

			Some parameters were defined through a simple analyze:
			\begin{itemize}
				\item Height was defined as 10 meters based on object resolution, visualizing which would be the heigher altitude in which it is still possible to identify the smallest object of interest, a bottle
				\item Speed was defined as 15km/h based on the quantity of frames that contains the whole biggest object, a pool
			\end{itemize}
		\end{frame}


		\begin{frame}\frametitle{Flight plan details}
			We decided to use a trajectory that:
			\begin{itemize}
				\item covers an rectangular area, defined as the rectangle with minimum area that surrounds the given points of interest
				\item considers the horizontal GPS positioning accuracy, expanding the rectangular area to be sure to cover all the given points
				\item maps the rectangular area in a ``boustrophedon'' way, i.e., from right to left and from left to right in alternate parallel lines
				\item has an overlap that allows an object of interest not to be cut in at least one pass, considering its maximum size and aircraft vision positioning accuracy
				\item the drone does not change its orientation, only its direction
			\end{itemize}
		\end{frame}


		\begin{frame}
			\frametitle{Some measures to meet the specifications}
			\begin{figure}
				\centering
				\includegraphics[height=6.4cm]{flightplan.png}
			\end{figure}
		\end{frame}


		\begin{frame}
			\frametitle{Example}
			\begin{figure}
				\centering
				\includegraphics[height=8cm]{gremio.pdf}
			\end{figure}
		\end{frame}


		\begin{frame}
			\frametitle{Flight mission}
			\begin{figure}
				\centering
				\includegraphics[width=12cm]{missionlitchi.png}
			\end{figure}
		\end{frame}


		\begin{frame}
			\frametitle{}
			\begin{enumerate}
				\item Select the points of interest on Google Maps or Litchi Mission Hub
				\item Define drone height (desired 10m) and speed (suggested 15km/h)
				\item Project the flight plan remotely
				\item Import it to mission hub and save it after verify general parameters:
				\begin{itemize}
					\item Finish Action: RTH (return to home)
					\item Path Mode: Straight Lines
					\item Default Gimbal Pitch Mode: Disabled
				\end{itemize}
				\item Arrange the objects in the area, if necessary
				\item Verify other parameters such as:
				\begin{itemize}
					\item Resolution: 4K (3.840 x 2.160)
					\item Focus at infinity (do it manually by touching the screen on the horizon)
					\item White balance as sunny or cloudy, depending on the weather
				\end{itemize}
				\item Record video for camera calibration
				\item Open the mission on litchi app
				\item Press play
			\end{enumerate}
		\end{frame}


		\begin{frame}\frametitle{Projective Transformation}
			%
			\begin{itemize}
			 \item  A camera maps a point $\Mbf' = [X,Y,Z,1]^\Trm$ in the space into a point $\mbf' = [u,v,1]^\Trm$ through a projective transformation:
			 \begin{equation}
			\label{eq:projection}
			s\mbf' = \Asf[\Rsf~|~\tbf]\Mbf',
			\end{equation}
			\begin{itemize}
			 \item $s$ is an arbitrary scale factor;
			 \item $\Rsf$ and $\tbf$ are, respectively, the rotation matrix and the translation vector (extrinsic parameters), which relate the real-world coordinate system to the camera's coordinate system.
			\end{itemize}
			\end{itemize}
			%
			%
		\end{frame}

		\begin{frame}\frametitle{Pinhole Camera Model}
			%TODO: make my own image.
			%
			\begin{figure}[htb!]
				\center
				\includegraphics[width=.7\textwidth]{pinhole_camera_model.png}
				\label{fig:keypts_1} %\\[-0.2cm]
				\caption{Pinhole camera model. (source: OpenCV).}
				%\url{https://docs.opencv.org/3.4.1/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d}
				\vspace{-3.5mm}
				% \href{https://docs.opencv.org/3.4.1/d9/d0c/group__calib3d.html#ga3207604e4b1a1758aa66acb6ed5aa65d}{[ref.: OpenCV documentation.]}
				\label{fig:undistort}
			\end{figure}
		\end{frame}


		\begin{frame}
			\frametitle{Intrinsic Parameters}
			\begin{itemize}
			 \item $\Asf$ is the the camera calibration matrix (intrinsic parameters):
			 \begin{equation}
			\Asf =
			\begin{bmatrix}
			\alpha & \gamma & u_0\\
			0 & \beta  & v_0\\
			0 &     0  & 1\\
			\end{bmatrix},
			\end{equation}
			\begin{itemize}
			 \item $[u_0,v_0]^\Trm$ denote the coordinates of the principal point,
			 \item $\alpha$ and $\beta$ the scaling factors in the $ u $ and $ v $ axes of the image, respectively,
			 \item $\gamma$ is the obliquity (degree of shear) of the two axes of the image.
			\end{itemize}
			\end{itemize}
		\end{frame}


		\begin{frame}\frametitle{Radial Distortion Compensation}
			\begin{itemize}
			 \item Conventional cameras usually have significant lens distortions, especially radial distortion.
			 \item Formulation:
			 \begin{itemize}
			  \item Let $(u,v)$ be the the ideal coordinates (free of distortion) of the pixel on the image, and
			  \item $(\breve{u},\breve{v})$ the actual coordinates of the observed image,
			  \item the ideal points are the projections of the points of the calibration standard according to the model given by Eq.~\eqref{eq:projection}.
			  \item Analogously, $(x,y)$ and $(\breve{x},\breve{y})$ are the normalized coordinates in the ideal (free of distortion) and real (distorted) images, respectively.
			  \item The radial distortion can be modeled as:
			\begin{align}
			\breve{x} &= x + x(k_1 r^2 + k_2 r^4) \\% + k_3 r^6)\\% + 2p_1 x y + p_2(r^2 + 2x^2), \\
			\breve{y} &= y + y(k_1 r^2 + k_2 r^4), % + k_3 r^6),% + p_1(r^2 + 2y^2) + 2 p_2 x y,
			\end{align}
			\item $k_1$ and $k_2$ are the radial distortion coefficients,
			\item $r^2 = (x^2 + y^2)$.
			 \end{itemize}
			\end{itemize}
		\end{frame}


		\begin{frame}
			\frametitle{Radial Distortion Compensation}
			%
			\begin{itemize}
			 \item The center of the radial distortion is located at the main point.
			 \item From $\breve{u} = \alpha \breve{x} + \gamma \breve{y} + u_0 $ and $\breve{v} = \beta \breve{y} + v_0$, and assuming $\gamma = 0$:
			  \begin{align}
			    \label{eq:u_dist}
			    \breve{u} &= u + (u-u_0)(k_1 r^2 + k_2 r^4 ) \\
			    \label{eq:v_dist}
			    \breve{v} &= v + (v-v_0)(k_1 r^2 + k_2 r^4 ).
			  \end{align}
			\end{itemize}
		\end{frame}


		\begin{frame}
			\frametitle{Zhang's Method}
			%
			\begin{itemize}
			 \item Make a first rough estimate of the intrinsic and extrinsic parameters of the camera
			 \item refine it by estimating the maximum likelihood
			 \begin{itemize}
			  \item Given $ n $ images of the calibration standard and considering $ m $ points in this standard,
			  \item assuming that the point images are corrupted by an i.i.d. noise
			  \item the maximum likelihood estimate can be obtained by minimizing the functional
			  %
			  \begin{equation}
			    \sum_{i=1}^{n}\sum_{j=1}^{m} \Vert \xbf_{ij} - \breve{\xbf}(\Asf, k_1, k_2, \Rsf_i, \tbf_i, \Xbf_j) \Vert^2
			    \label{eq:functional},
			  \end{equation}
			  \item $\breve{\xbf}(\Asf, k_1, k_2, \Rsf_i, \tbf_i, \Xbf_j)$ is the projection of point $\Xbf_j$, according to Eq.~\eqref{eq:projection},
			followed by the distortion modeled according to the Eqs.~\eqref{eq:u_dist} and~\eqref{eq:v_dist}
			  \item Minimize the functional in~\eqref{eq:functional} is a nonlinear optimization problem that can be solved through the algorithm of Levenberg-Marquardt.
			 \end{itemize}
			\end{itemize}
			%
		\end{frame}


		\begin{frame}\frametitle{Comparison: distorted frame $\times$ undistorted}
			\begin{figure}[htb!]
				\center
				\subfloat[]{\includegraphics[width=0.5\textwidth]{frame_420_pts.png}
					\label{fig:keypts_1}} %\\[-0.2cm]
					\subfloat[]{\includegraphics[width=0.5\textwidth]{frame_420_undistorted.png}%
					\label{fig:undistort_1}} %\\[-0.2cm]
				\label{fig:undistort}
			\end{figure}
		\end{frame}


		\begin{frame}\frametitle{Annotation}
			\begin{itemize}
				\item The frame-by-frame annotation of all acquired sequences is done with the Zframer system, as illustrated below.
			\end{itemize}
			\begin{figure}[htb]
				\centering
				\includegraphics[width=.98\columnwidth]{zframer_marking.png}
				\label{fig:zframer1}
			\end{figure}
		\end{frame}







		\begin{frame}\frametitle{Objects}
			\begin{figure}[htb]
				\centering
				\includegraphics[width=.1\linewidth]{garrafa1.png}
				\includegraphics[width=.2\linewidth]{garrafa2.png}
				\includegraphics[width=.1\linewidth]{pneu1.png}
				\includegraphics[width=.1\linewidth]{pneu3.png}\\
				\includegraphics[width=.1\linewidth]{pneu5.png}
				\includegraphics[width=.1\linewidth]{water1.png}
				\includegraphics[width=.1\linewidth]{water2.png}
				\includegraphics[width=.1\linewidth]{water3.png}
			% \includegraphics[width=.26\linewidth]{base4.png}
				\label{fig:objetos1}
			\end{figure}


		\end{frame}


	\subsection{Scenarios}

		\begin{frame}\frametitle{Scenarios}
			\begin{figure}[htb!]
				\centering
				\includegraphics[width=.35\linewidth]{base1.png}
				\includegraphics[width=.36\linewidth]{base2.png}\\
				\includegraphics[width=.35\linewidth]{base3.png}
			% \includegraphics[width=.26\linewidth]{../figures/base4.png}
				\label{fig:scenarios1}
			\end{figure}

		\end{frame}


\subsection{Camera Calibration: Zhang's Method}

% -----------------------------------------------------------------------------
% \frame{
%   \frametitle{Projection}
%   %
%   Considera-se que uma câmera mapeia um ponto $\Mbf' = [X,Y,Z,1]^\Trm$ do espaço no ponto $\mbf' = [u,v,1]^\Trm$ da imagem através de uma transformação projetiva da forma:
%   %
%   \begin{equation}
%   \label{eq:projection}
%   s\mbf' = \Asf[\Rsf~|~\tbf]\Mbf',
%   \end{equation}
%   %
%   onde $s$ é um fator de escala arbitrário, $\Rsf$ e $\tbf$ são, respectivamente, a matriz de rotação e o vetor de translação (parâmetros extrínsecos),que relacionam o sistema de coordenadas do mundo real com o sistema de coordenadas da câmera.
%   %
%   %
% }

% -----------------------------------------------------------------------------


% -----------------------------------------------------------------------------
% \frame{
%   \frametitle{Parâmetros Intrínsecos}
%     $\Asf$ é a matriz de calibração da câmera (parâmetros intrínsecos), definida por:
%     \begin{equation}
%       \Asf =
%       \begin{bmatrix}
% 	\alpha & \gamma & u_0\\
% 	0 & \beta  & v_0\\
% 	0 &     0  & 1\\
%       \end{bmatrix},
%     \end{equation}
%     com $[u_0,v_0]^\Trm$ denotando as coordenadas do ponto principal,
%     $\alpha$ e $\beta$ os fatores de escala nos eixos $u$ e $v$ da imagem, respectivamente,
%     e $\gamma$ é a obliquidade (grau de cisalhamento) dos dois eixos da imagem.
% }

% -----------------------------------------------------------------------------
% \frame{
%   \frametitle{Compensação de Distorção Radial}
%    Câmeras convencionais geralmente possuem significantes distorções de lentes, especialmente distorção radial.
%
%    Sejam $(u,v)$ as coordenadas ideais (livre de distorção) do pixel na imagem e $(\breve{u},\breve{v})$ as coordenadas correspondentes reais de imagem observada.
%
%     Os pontos ideais são as projeções dos pontos do padrão de calibração de acordo com o modelo dado pela Eq.~\eqref{eq:projection}.
%
%    Analogamente, $(x,y)$ e $(\breve{x},\breve{y})$ são as coordenadas normalizadas nas imagens ideal (livre de distorção) e real (distorcida), respectivamente.
%
%    A distorção radial pode ser modelada como:
%    \begin{align}
%     \breve{x} &= x + x(k_1 r^2 + k_2 r^4) \\% + k_3 r^6)\\% + 2p_1 x y + p_2(r^2 + 2x^2), \\
%     \breve{y} &= y + y(k_1 r^2 + k_2 r^4), % + k_3 r^6),% + p_1(r^2 + 2y^2) + 2 p_2 x y,
%    \end{align}
% onde $k_1$ e $k_2$ são os coeficientes de distorção radial e $r^2 = (x^2 + y^2)$.
%
%
% }

% -----------------------------------------------------------------------------
% \frame{
%   \frametitle{Compensação de Distorção Radial}
%     %
%     O centro da distorção radial está localizado no ponto principal.
%
%     De $\breve{u} = \alpha \breve{x} + \gamma \breve{y} + u_0 $ e $\breve{v} = \beta \breve{y} + v_0$, assumindo $\gamma = 0$,
%     pode-se escrever que
%     %
%     \begin{align}
%       \label{eq:u_dist}
%       \breve{u} &= u + (u-u_0)(k_1 r^2 + k_2 r^4 ) \\
%       \label{eq:v_dist}
%       \breve{v} &= v + (v-v_0)(k_1 r^2 + k_2 r^4 ).
%     \end{align}
%
%
% }

% -----------------------------------------------------------------------------
% \frame{
%    \frametitle{Método de Zhang}
%     %
%     O método de Zhang consiste em fazer, primeiramente, uma estimativa grosseira dos parâmetros intrínsecos e extrínsecos da câmera para depois refiná-la através da estimativa da máxima verossimilhança.
%
%     Dadas $n$ imagens do padrão de calibração e considerando $m$ pontos neste padrão,
%     assumindo que as imagens dos pontos estão corrompidas por um ruído independente e identicamente distribuído, a estimativa de máxima verossimilhança pode ser obtida minimizando o funcional
%     %
%     \begin{equation}
%       \sum_{i=1}^{n}\sum_{j=1}^{m} \Vert \xbf_{ij} - \breve{\xbf}(\Asf, k_1, k_2, \Rsf_i, \tbf_i, \Xbf_j) \Vert^2
%       \label{eq:functional},
%     \end{equation}
%     %
%     onde $\breve{\xbf}(\Asf, k_1, k_2, \Rsf_i, \tbf_i, \Xbf_j)$ é a projeção do ponto $\Xbf_j$, de acordo com a Eq.~\eqref{eq:projection},
%     seguido da distorção modelada de acordo com as Eqs.~\eqref{eq:u_dist} e~\eqref{eq:v_dist}.
%
%     Minimizar o funcional em~\eqref{eq:functional} é um problema de otimização não-linear que pode ser resolvido através do algoritmo de Levenberg-Marquardt.
%
% }

% -----------------------------------------------------------------------------






%%==============================================================================
%% SECTION - Object Detection with Deep Learning
%%==============================================================================
	\section{Object Detection with Deep Learning}

		\subsection{Deep Learning}

			\begin{frame}\frametitle{Convolutional Neural Networks}
				\begin{figure}[htb]
					\centering
					\includegraphics[width=.4\columnwidth]{conv_eng.pdf}
					\includegraphics[width=6.5cm]{drone-persp2.png}
					\label{fig:zframer1}
				\end{figure}
			\end{frame}

%%==============================================================================
%% SECTION - Results and Discussions
%%==============================================================================
	\section{Results and Discussions}

		\subsection{Evaluation}

			\begin{frame}\frametitle{Intersection over Union (IoU)}
				\begin{itemize}
					\item Evaluate the model regarding object localization;
					\item Detections are considered as TP if the IoU $\geq$ threshold and as FP otherwise.
				\end{itemize}
				%
				\begin{figure}[htb]
					\centering
					\includegraphics[width=.6\linewidth]{IoU.pdf}
					\label{fig:IoU}
				\end{figure}

			\end{frame}


		\subsection{Quantitative results}

		\begin{frame}\frametitle{Results for CEFET dataset}
			\begin{table}[b!]
				\centering
				\resizebox{\textwidth}{!}{%
				\begin{tabular}{@{}c|l|c|ccc|cc@{}}
				\toprule
															 &                        & backbone & mAP   & mAP$_{50}$ & mAP$_{75}$ & mAP$_{\rm M}$ & mAP$_{\rm L}$ \\
															 \hline
				\multirow{3}{*}{train} & Faster R-CNN (no aug.) & R-50-C4 &  89.86 & 90.19 & 90.19 & 86.15 & 92.16 \\
															 & Faster R-CNN           & R-50-C4 & 89.11 & 89.84 & 89.84 &  87.32 & 90.34  \\
															 & Faster R-CNN           & R-101-C4 & 88.95 & 89.48 & 89.48 &  87.52 & 91.02 \\
				\hline
				\multirow{3}{*}{test}  & Faster R-CNN (no aug.) & R-50-C4  & 43.81 & 62.25 & 53.64 &  34.46 & 59.56  \\
															 & Faster R-CNN           & R-50-C4  & 47.38 & 64.16 & 57.70 &  38.42 & 61.85\\
															 & Faster R-CNN           & R-101-C4 & 49.31 & 66.68 & 62.61 &  39.46 & 65.21 \\
				\bottomrule
				\end{tabular}
				}
				\label{tab:results_CEFET}
				\end{table}
		\end{frame}


		\begin{frame}%\frametitle{Quantitative results}
			\begin{figure}[htb!]
				\centering
				\includegraphics[width=.35\linewidth, trim={0 0 0 0},clip]{pr_curve_e2e_faster_rcnn_R_101_C4_1x_cocostyle_zoom_1.pdf}~
				\includegraphics[width=.35\linewidth, trim={0 0 0 0},clip]{pr_curve_e2e_faster_rcnn_R_101_C4_1x_cocostyle_zoom_2.pdf}\\
				\vspace{.5mm}
				\includegraphics[width=.7\linewidth, trim={0 0 0 0},clip]{pr_curve_e2e_faster_rcnn_R_101_C4_1x_cocostyle.pdf}
			% 	\includegraphics[width=.5\linewidth]{base4.png}
				% \caption{Precision-recall curve for R-101-C4 at various IoUs.}
				\label{fig:pr_R101C4}
			\end{figure}
			%
			%
		\end{frame}


		\begin{frame}%\frametitle{Quantitative results}
			\begin{figure}[htb!]
				\centering
				\includegraphics[width=.51\linewidth]{pr_curve_comp_50.pdf}
				\includegraphics[width=.51\linewidth]{pr_curve_comp_75.pdf}
				% \caption{Precision-recall curve at IoU = 0.50.}
				% \caption{Precision-recall curve at IoU = 0.75.}
				\label{fig:prec-rec_curve}
			\end{figure}
			%
			%
			%
		\end{frame}


	\subsection{Visual analysis}

		\begin{frame}\frametitle{Hard example}
			\begin{figure}[h!]
				\centering
				\includegraphics[width=.55\textwidth,trim={0 2.9cm 0 2.4cm},clip]{_vis_results/e2e_faster_rcnn_R_101_C4_1x_cocostyle/img_30.pdf}
				% \caption{Hard example.}
				\label{fig:hard}
			\end{figure}
			%
		\end{frame}


		\begin{frame}\frametitle{Comparison among models}
			\begin{figure}[htb!]
				\centering
				\includegraphics[width=.5\textwidth,trim={0 2.9cm 0 2.4cm},clip]{_vis_results/e2e_faster_rcnn_R_50_C4_1x_cocostyle_NO-AUG/img_1.pdf}
				\includegraphics[width=.5\textwidth,trim={0 2.9cm 0 2.4cm},clip]{_vis_results/e2e_faster_rcnn_R_50_C4_1x_cocostyle/img_1.pdf}\\
				\vspace{3mm}
				\includegraphics[width=.5\textwidth,trim={0 2.9cm 0 2.4cm},clip]{_vis_results/e2e_faster_rcnn_R_101_C4_1x_cocostyle/img_1.pdf}
				% \caption{Detection improvement over the models.}
  			\label{fig:improv_1}
			\end{figure}
		\end{frame}


		\begin{frame}\frametitle{Comparison among models}
			\begin{figure}[htb!]
				\centering
				\includegraphics[width=.5\textwidth,trim={0 2.9cm 0 2.4cm},clip]{_vis_results/e2e_faster_rcnn_R_50_C4_1x_cocostyle_NO-AUG/img_12.pdf}
				\includegraphics[width=.5\textwidth,trim={0 2.9cm 0 2.4cm},clip]{_vis_results/e2e_faster_rcnn_R_50_C4_1x_cocostyle/img_12.pdf}
				\vspace{3mm}
				\includegraphics[width=.5\textwidth,trim={0 2.9cm 0 2.4cm},clip]{_vis_results/e2e_faster_rcnn_R_101_C4_1x_cocostyle/img_12.pdf}
				% \caption{Another detection improvement over the models.}
  			\label{fig:improv_2}
			\end{figure}
		\end{frame}


		\begin{frame}\frametitle{Some false positives cases}
			\begin{figure}[htb!]
				\centering
				\includegraphics[width=.5\textwidth,trim={0 0 0 0},clip]{_vis_results/e2e_faster_rcnn_R_50_C4_1x_cocostyle_NO-AUG/img_78.pdf}
				\includegraphics[width=.5\textwidth,trim={0 0 0 0},clip]{_vis_results/e2e_faster_rcnn_R_50_C4_1x_cocostyle/img_78.pdf}
				\vspace{.1mm}
				\includegraphics[width=.5\textwidth,trim={0 0 0 0},clip]{_vis_results/e2e_faster_rcnn_R_101_C4_1x_cocostyle/img_78.pdf}
				% \caption{Some false positives cases.}
				\label{fig:FP_cases}
			\end{figure}
		\end{frame}


		\begin{frame}\frametitle{High occlusion example}
			\begin{figure}[htb!]
				\centering
				\includegraphics[width=.5\textwidth,trim={0 0 0 0},clip]{_vis_results/e2e_faster_rcnn_R_50_C4_1x_cocostyle_NO-AUG/img_127.pdf}
				\includegraphics[width=.5\textwidth,trim={0 0 0 0},clip]{_vis_results/e2e_faster_rcnn_R_50_C4_1x_cocostyle/img_127.pdf}
				\vspace{.1mm}
				\includegraphics[width=.5\textwidth,trim={0 0 0 0},clip]{_vis_results/e2e_faster_rcnn_R_101_C4_1x_cocostyle/img_127.pdf}
				% \caption{High occlusion example.}
  			\label{fig:occlusion}
			\end{figure}
		\end{frame}


		\begin{frame}\frametitle{Wrong false positive}
			\begin{figure}[h!]
				\centering
				\includegraphics[width=.8\textwidth,trim={0 0 0 0},clip]{_vis_results/e2e_faster_rcnn_R_50_C4_1x_cocostyle_NO-AUG/img_133.pdf}
				% \caption{Wrong false positive.}
				\label{fig:wrong_fp}
			\end{figure}
			%
		\end{frame}

%%==============================================================================
%% SECTION - Conclusion and Future Works
%%==============================================================================
	\section{Conclusion and Future Works}

		\begin{frame}
			\frametitle{Conclusion}
			\begin{itemize}
				\item To deliver a decision support system
				\item To indicate your geographic positions
				\item To facilitate scan abandoned or blocked areas
			\end{itemize}

		\begin{itemize}
			\item This work described the problem of automatic detection, by computer vision techniques, of mosquitoes breeding sites.
			\item We pointed to the need to create a database to attack the problem, containing videos with several objects that accumulate clean water scattered in several scenarios.
			\item For this, the use of a VANT was considered, which allows the acquisition of videos covering a wide geographical area.
			\item All videos are manually annotated with the help of free software, allowing the training of different critical object detector algorithms for the application of interest.
		\end{itemize}

		\end{frame}

		\begin{frame}
			\frametitle{THANK YOU!}
			\centering
			Wesley Lobato Passos\\
			wesley.lpassos@gmail.com
		\end{frame}



\end{document}